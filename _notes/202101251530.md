---
layout: post
comments: True
date: 20210125
title: What is the best performance fix you ever achieved by changing only a few lines of code?
source_url: https://www.reddit.com/r/datascience/comments/knbhvp/what_is_the_best_performance_fix_you_ever/
topics: [[[pandas]], [[Data Science]]]
tags: [literature]
status: in-progress
---

-   This is more of a general rule: if you're using for loops in combination with Pandas, there's likely a way in pure Pandas for a performance improvement. Usually it's the first thing I go through when refactoring code.
    -   Pandas and numpy has a lot of built-in functions that are compiled (and sometimes use SIMD etc. under the hood).
    -   if you're implementing an algorithm or doing something complicated, it actually makes sense to keep it simple using loops and built-in python stuff and compiling it instead of trying to use numpy/pandas.
    -   The reason is simple: compilers are better at optimizing code than you are.
    -   The JIT compiler handles it.
-   if you are using SQL learn indexes, if you're using HDFS learn partitions
-   from ~2 min per run to ~8 seconds by, at various points in time:
    -   swapping out indexing a pandas dataframe (which is suprisingly slow) to indexing a list of dict [code-0]
    -   wrapping a [cache decorator](https://docs.python.org/3/library/functools.html#functools.lru_cache) around some hot functions that get called redundantly
    -   thowing some [numba jit](https://numba.pydata.org/) decorators around some hot matrix operations
    -   A weird one was telling numpy to [stop using so many threads](https://stackoverflow.com/questions/30791550/limit-number-of-threads-in-numpy), which was actually slowing down the code due to the overhead of the threads
    -   building pandas masks up incrementally rather than doing it in the inner loop
    -   a whole bunch of small, but effective optimizations that can be summarised as "stop doing the same thing every time inside a for loop, instead, do it before the for loop" - this was sometimes obsfucated by function calls
    -   some cases of "track and save the data now when it is cheap rather than recalculating it later on"
    -   #RL: [[Profiling Python using cProfile: a concrete case]]
-   comparing two different types, make both of them the same
-   Using Pythonâ€™s `set` instead of a `list` for common `if var in container` checks. Really makes a difference if you have a bunch of items!
-   CPU to GPU with RAPIDS
    -   Pandas - > cudf
    -   numpy -> cupy
    -   networkx -> cugraph


[code-0]

```python
	# before, surprisingly slow
	df = pd.DataFrame.from_dict([{"a": 1, "b": 2}, {"a": 3, "b": 4}])
	for idx in range(len(df)):
		datum = df.iloc[idx]
		# do stuff
	# after, noticeably faster
	data = [{"a": 1, "b": 2}, {"a": 3, "b": 4}]
	for idx in range(len(df)):
		datum = datum[idx]
		# do stuff
	# enumerate is faster
	for idx, datum in enumerate(data):
		# do stuff
```
