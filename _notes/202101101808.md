---
layout: post
comments: True
date: 20210110
title: Lecture 0
source_url: https://drive.google.com/drive/u/0/folders/1lFRRUJ1EHWxvaPNIWwQUMmU5Esom7Nkz
topics: [[[Matrix Analysis]]]
tags: [slides]
status: in-progress
---

## Course

-   Structure of the course
    -   W1: basic matrix concepts, subspace, norms
    -   W2: [[Linear Least Squares]] (LS)
    -   W3: [[Eigendecomposition]]
        -   PageRank
    -   W4: [[Singular Value Decomposition]]
    -   W5 [[Positive Semidefinite Matrices]]
    -   W6: [[Linear System of Equations]]
        -   other form of LS
    -   W7: [[Compressive Matrix Sensing]]
        -   new
    -   W8: [[Nonnegative Factorization]]
        -   core topic
    -   W9: [[Tensor Decomposition]]
        -   high dimension linear algebra
-   Learning Resources
    -   [[Matrix Computations by Golub]] **MUST**
        -   use as a dictionary
        -   how to decompose matrix algorithms
        -   for math
        -   go-to book for matrix for Machine Learning and Signal Processing
    -   [[Matrix Analysis by Horn]]
        -   for math
        -   explains theory
    -   [[ECE 712 Course Notes by Reilly]]
        -   for engineers
        -   more about Signal Processing applications (detailed)

## [[Least Squares]] (LS)

-   Problem: given A (mxn), y (n), solve
    $$
    \min _{\mathbf{x} \in \mathbb{R}^{n}}\|\mathbf{y}-\mathbf{A} \mathbf{x}\|_{2}^{2}
    $$
-   Eucildean norm:
    $$
    \|\mathbf{x}\|_{2}=\sqrt{\sum_{i=1}^{n}\left|x_{i}\right|^{2}}
    $$
-   Assuming a tall and full-rank A, the LS solution is uniquely given by: - _closed-form solution?_
    $$
    \mathbf{x}_{\mathrm{LS}}=\left(\mathbf{A}^{T} \mathbf{A}\right)^{-1} \mathbf{A}^{T} \mathbf{y}
    $$

### Autoregression (AR) model

-   Application example: Linear Prediction
    $$
    	y[n]=a_{1} y[n-1]+a_{2} y[n-2]+\cdots+a_{L} y[n-L]+w[n]
    $$
-   $\left\{a_{i}\right\}_{i=1}^{L}$ are some coefficients
-   $w[n]$ is noise
-   Problem:
    $$
    \text { estimate }\left\{a_{i}\right\}_{i=1}^{L} \text { from }\{y[n]\}
    $$
-   Example: Predicting [[Hang Seng Index]]
    -   The stock market in Hong Kong
    -   ![](https://cdn.mathpix.com/snip/images/MjPbHU9HYuW5N1DVGS5WLABoEQMDL6wXI-Iq4yWvn-Q.original.fullsize.png)
-   Example: Real-time Predication of Flu
    -   _Final Project on Covid data?_
    -   ARGO, a model combining the AR model and Google search data [Yang-Santillana-Kou2015](https://drive.google.com/file/d/18Q6rFzaEgdiyJxx7UfphMCt_WZPBlh1C/view?usp=sharing)

## Eigenvalue Problem

-   Problem: given A (nxn), find v (n) such that
    $$
    \mathbf{A} \mathbf{v}=\lambda \mathbf{v}
    $$

### Eigendecomposition:

-   let A be symmetric
-   admits a decomposition

    $$
    \mathbf{A}=\mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^{T}
    $$

-   Q is orthogonal (i.e. $\mathbf{Q}^{T} \mathbf{Q}=\mathbf{I}$)
-   $\boldsymbol{\Lambda}=\operatorname{Diag}\left(\lambda_{1}, \ldots, \lambda_{n}\right)$
-   no closed form in general, but can be numerically computed
    -   _what does this mean?_

### PageRank

-   Application example of Eigenvalue problem
-   PageRank used to rank the pages of Google search results
-   uses counts of links of various pages to determine pages' importance

### One-Page Explanation of How PageRank Works

-   Model:
    $$
    \sum_{j \in \mathcal{L}_{i}} \frac{v_{j}}{c_{j}}=v_{i}
    $$
-   $c_j$ is the number of outgoing links from page j
-   $L_i$ is set of pages with a link to page i
-   $v_i$ is the importance score of page i
-   [Bryan-Tanya2006](https://drive.google.com/file/d/1A1c2ziC-RqVfc8q5LhnNP8V20e04brMd/view?usp=sharing)
-   questions
    -   _how to define it?_
    -   _how to generate the solution (what if it doesn't exist?)_
    -   _how to compute?_

## [[Low-Rank Matrix Approximation]]

-   Needed when reduce noise, dimension reduction (most basic useful one)
-   Problem: Y (mxn) is very large matrixr < min(m,n), find an (A,B) in (mxr)x(rxn)
    $$
    \text { such that either } \mathbf{Y}=\mathbf{A B} \text { or } \mathbf{Y} \approx \mathbf{A B}
    $$
-   Y, large matrices tend to have noise
-   AB, factorization
-   B, latent representation learning
    -   _what is this?_
-   r, rank (low-rank = small as possible)
-   Formulation:
    $$
    \min _{\mathbf{A} \in \mathbb{R}^{m \times r}, \mathbf{B} \in \mathbb{R}^{r \times n}}\|\mathbf{Y}-\mathbf{A B}\|_{F}^{2}
    $$

### [[Image Compression]]

-   Application of Low-rank Matrix Approximation
-   let Y (mxn)
-   store the low-rank factor pair (A,B), instead of Y
    -   _what is the relationship between rank and matrices?_
        -   if there is no definition of rank, there isn't any difference between vectors and matrices
            -   _what?_
-   memory cost
    -   from O(mn) to O((m+n)r)
        -   _what is this O? It is written differently_

### [[Principal Component Analysis]] (PCA)

-   Application of Low-rank Matrix Approximation
-   Problem: given {y_1, y_2, ..., y_n} in (n) and k < min(m,n), perform a low-dimensional representation
    $$
    \mathbf{y}_{i}=\mathbf{Q} \mathbf{c}_{i}+\boldsymbol{\mu}+\mathbf{e}_{i}
    $$
-   Q (mxk) is a basis
    -   _what is a basis?_
-   c_i's are coefficients
    -   _what is a coefficient?_
-   $\mu$ is a base
    -   _what is a base?_
-   e_i's are errors
-   PCA (in reduction of a face image dataset) produces a set of singular vectors (1st-400th) where each one contains a certain average of faces
    -   _so take the first 150 rank elements and discard the rest?_
    -   _higher order = lower importance?_

### [[Singular Value Decomposition]] (SVD)

-   SVD: any Y (mxn) can be decomposed into
    $$
    \mathbf{Y}=\mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^{T}
    $$
-   U (mxm) and V (nxn) are orthogonal
-   $\Sigma$ (mxn) takes a diagonal form
-   SVD solves the low-rank matrix approximation problem

## [[Linear System of Equations]]

-   Problem: given A (nxn), y (n), solve:
    $$
    \mathbf{A x}=\mathbf{y}
    $$
-   _what if your data (x) and label (y) have noise? How sensitive is your model?_
-   Questions
    -   How to solve it?
    -   How to solve it when n is very large?
    -   How sensitive is the solution x when A and y contain errors?

## [[The Sparse Recovery Problem]]

-   Problem: given y (m), A (mxn), m < n, find sparsest x (n) such that
    $$
    \mathbf{A x}=\mathbf{y}
    $$
-   sparsest = x should have as many zero elements as possible

### Magnetic resonance imaging (MRI)

-   Application of The Sparse Recovery Problem
-   Problem: MRI image reconstruction
-   image -> frequency domain using Fourier coefficients -> recovery by filling unobserved Fourier coefficients to zero [Cand'es-Romberg-Tao2006](https://drive.google.com/file/d/1RPEgW46frzokBSBtRdyzDAzMuvVTR10P/view?usp=sharing)
    -   _unobserved Fourier coefficients?_

## [[Low-Rank Matrix Completion]]

-   Application: recommender systems
-   Z be a preference matrix, where $z_{ij}$ records how user i likes movie j
-   some $z_{ij}$ are missing since no one watches all movies
-   Z is assumed to be low-tank (only a few factors affect users' preferences)
-   Goal: guess the unkown $z_{ij}$ from the known ones
-   Questions:
    -   _what if every row is i.i.d, what is the rank?_
        -   m, probability of 1
    -   rows and columns are collated, hence low-rank works
-   the winners of 2009 Netflix Grand Prize used low-rank matrix approximation [Koren-Bell-Volinsky2009](https://drive.google.com/file/d/10Q-52U7_B1q5qTmcLG2YICtCMa3yP204/view?usp=sharing)
-   Formulation (oversimplified):
    $$
    \min _{\mathbf{A} \in \mathbb{R}^{m \times r}, \mathbf{B} \in \mathbb{R}^{r \times n}} \sum_{(i, j) \in \Omega}\left|z_{i j}-[\mathbf{A B}]_{i, j}\right|^{2}
    $$
-   $\Omega$ is an index set that indicates the known entries of Z
-   can't be solved by SVD
-   alternating LS may be used

### Image Denoising Problem

-   Application of Low-Rank Matrix Completion

## [[Nonnegative Matrix Factorization]] (NMF)

-   Goal: factors to be non-negative
-   Formulation:
    $$
    \min _{\mathbf{A} \in \mathbb{R}^{m \times r}, \mathbf{B} \in \mathbb{R}^{r \times n}}\|\mathbf{Y}-\mathbf{A B}\|_{F}^{2} \quad \text { s.t. } \mathbf{A} \geq \mathbf{0}, \mathbf{B} \geq \mathbf{0}
    $$
-   able to extract meaningful features (by empirical studies)
    -   _how? And what is an empirical study?_

### Image Processing

-   basis elements extract facial features (e.g. eyes, nose, and lips) [Lee-Seung1999](https://drive.google.com/file/d/1bVvyp6Yc5gtBfkQEQlG-e8fS5loD5KlL/view?usp=sharing)

### Text Mining

$Y=AB$

-   A is the dictionary and B is the set of weights
-   basis elements recover different topics
-   weights assign each text to its corresponding topics

### Face Recognition

-   NMF-Extracted Features are sparse
-   every set correspond to a facial feature (e.g. nose, forehead)
-   how much weights for nose, forehead, etc.
-   _NMF has most matrices 0s?_
    -   yes!

## NMF vs. PCA in Face Recognition

-   NMF is sparse whereas PCA isn't, which means NMF is more effective
    -   _NMF requires more train?_
